{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ericpts/.local/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import gin\n",
    "import lib_analysis\n",
    "import lib_biased_mnist\n",
    "import lib_plot\n",
    "import lib_problem\n",
    "import lib_toy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import yaml\n",
    "from jupyter_dash import JupyterDash \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "external_stylesheets = [\"https://codepen.io/chriddyp/pen/bWLwgP.css\"]\n",
    "\n",
    "tqdm.pandas()\n",
    "gin.enter_interactive_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"config.gin\")\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "problem = lib_biased_mnist.BiasedMnistProblem()\n",
    "\n",
    "in_dist = tf.data.Dataset.from_tensor_slices(\n",
    "    problem.filter_tensors(\n",
    "        *lib_biased_mnist.get_biased_mnist_data(\"~/.datasets/mnist/\", 1.0, train=False)\n",
    "    )\n",
    ").cache()\n",
    "\n",
    "oo_dist = tf.data.Dataset.from_tensor_slices(\n",
    "    problem.filter_tensors(\n",
    "        *lib_biased_mnist.get_biased_mnist_data(\"~/.datasets/mnist/\", 0.1, train=False)\n",
    "    )\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generalization_error(X, y, y_biased, y_hat):\n",
    "    per_digit_accuracy = lib_analysis.make_confusion_matrix(X, y, y_biased, y_hat)\n",
    "    errors = []\n",
    "    for bg in range(10):\n",
    "        per_digit = [None] * 2\n",
    "\n",
    "        for digit in range(2):\n",
    "            n_correct, n_total = per_digit_accuracy[digit, bg]\n",
    "            if n_total == 0:\n",
    "                per_digit[digit] = 0.0\n",
    "            else:\n",
    "                per_digit[digit] = n_correct / n_total\n",
    "\n",
    "        e = (per_digit[0] - per_digit[1]) ** 2.0\n",
    "        errors.append(e)\n",
    "    mse = np.mean(errors)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row: pd.DataFrame, root: Path):\n",
    "    models = []\n",
    "    for p in row[\"model_paths\"]:\n",
    "        p = root / Path(p)\n",
    "        m = tf.keras.models.load_model(p, compile=False)\n",
    "        models.append(m)\n",
    "\n",
    "    batch_size = 1024\n",
    "    # in_X, in_y, in_y_biased, in_y_hat = process_dataset(in_dist, models, batch_size)\n",
    "    oo_X, oo_y, oo_y_biased, oo_y_hat = lib_analysis.process_dataset(oo_dist, models, batch_size)\n",
    "\n",
    "    ret = {}\n",
    "    per_network = sorted(\n",
    "        [\n",
    "            model_generalization_error(\n",
    "                oo_X, oo_y, oo_y_biased, tf.math.argmax(oo_y_hat, axis=-1)[:, im]\n",
    "            )\n",
    "            for im in range(2)\n",
    "        ]\n",
    "    )\n",
    "    for im in range(2):\n",
    "        ret[f\"network_{im}_generalization_error\"] = per_network[im]\n",
    "\n",
    "    ret[\"ensemble_generalization_error\"] = model_generalization_error(\n",
    "        oo_X,\n",
    "        oo_y,\n",
    "        oo_y_biased,\n",
    "        tf.math.argmax(tf.math.reduce_mean(oo_y_hat, axis=1), axis=-1),\n",
    "    )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_generalization_error(df: pd.DataFrame, root: Path) -> pd.DataFrame:\n",
    "    genzation = df.progress_apply(lambda row: process_row(row, root), axis=1)\n",
    "    keys = genzation[0].keys()\n",
    "    d = {k: [] for k in keys}\n",
    "\n",
    "    for g in genzation:\n",
    "        for k in keys:\n",
    "            d[k].append(g[k])\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame(d)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_generalization_rows(row: pd.DataFrame):\n",
    "    df = []\n",
    "    for col in [\"network_0\", \"network_1\", \"ensemble\"]:\n",
    "        row_name = f\"{col}_generalization_error\"\n",
    "        d = row[row_name]\n",
    "        r = row.copy()\n",
    "        r[\"generalization_error\"] = d\n",
    "        r[\"generalization_error_model\"] = col\n",
    "        df.append(r)\n",
    "    df = pd.DataFrame(df)\n",
    "    for col in [\"network_0\", \"network_1\", \"ensemble\"]:\n",
    "        row_name = f\"{col}_generalization_error\"\n",
    "        df = df.drop(row_name, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"../data/33bdc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [02:31<00:00,  1.90it/s]\n",
      "100%|██████████| 288/288 [00:02<00:00, 124.07it/s]\n"
     ]
    }
   ],
   "source": [
    "DF = lib_analysis.read_problem(data_root, \"biased_mnist\")\n",
    "\n",
    "DF = DF[DF[\"BiasedMnistProblem.background_noise_level\"] == 0].reset_index(drop=True)\n",
    "\n",
    "DF = add_generalization_error(DF, data_root)\n",
    "DF = pd.concat(list(DF.progress_apply(expand_generalization_rows, axis=1))).reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_for_series(df: pd.Series):\n",
    "    models = []\n",
    "    for p in df[\"model_paths\"]:\n",
    "        p = data_root / Path(p)\n",
    "        m = tf.keras.models.load_model(p, compile=False)\n",
    "        models.append(m)\n",
    "\n",
    "    batch_size = 1024\n",
    "    # in_X, in_y, in_y_biased, in_y_hat = process_dataset(in_dist, models, batch_size)\n",
    "    oo_X, oo_y, oo_y_biased, oo_y_hat = lib_analysis.process_dataset(oo_dist, models, batch_size)\n",
    "    return lib_analysis.print_statistics(oo_X, oo_y, oo_y_biased, oo_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0f3ea053d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_level_tabs = []\n",
    "\n",
    "for bg_level in np.unique(DF[\"BiasedMnistProblem.background_noise_level\"]):\n",
    "    tabs_for_bg_level = []\n",
    "    for l_corr in np.unique(sorted(list(DF[\"label_correlation\"]), reverse=True)):\n",
    "\n",
    "        def stats_for_lambda(lambda_):\n",
    "            df = (\n",
    "                DF[\n",
    "                    (DF[\"label_correlation\"] == l_corr)\n",
    "                    & (DF[\"indep\"] == \"conditional_hsic\")\n",
    "                    & (DF[\"lambda\"] == lambda_)\n",
    "                    & (DF[\"BiasedMnistProblem.background_noise_level\"] == bg_level)\n",
    "                ]\n",
    "                .sample(1)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            df = df.iloc[0]\n",
    "\n",
    "            models = []\n",
    "            for p in df[\"model_paths\"]:\n",
    "                p = data_root / Path(p)\n",
    "                m = tf.keras.models.load_model(p, compile=False)\n",
    "                models.append(m)\n",
    "\n",
    "            batch_size = 1024\n",
    "            # in_X, in_y, in_y_biased, in_y_hat = process_dataset(in_dist, models, batch_size)\n",
    "            oo_X, oo_y, oo_y_biased, oo_y_hat = lib_analysis.process_dataset(\n",
    "                oo_dist, models, batch_size\n",
    "            )\n",
    "            return lib_analysis.print_statistics(oo_X, oo_y, oo_y_biased, oo_y_hat)\n",
    "\n",
    "        fig = lib_plot.end_to_end_plot(\n",
    "            go.Figure(),\n",
    "            DF[\n",
    "                (DF[\"label_correlation\"] == l_corr)\n",
    "                & (DF[\"BiasedMnistProblem.background_noise_level\"] == bg_level)\n",
    "            ].copy(),\n",
    "            \"generalization_error\",\n",
    "            [\"generalization_error_model\", \"indep\"],\n",
    "            f\"MSE between per-digit accuracies for label correlation {l_corr}; background noise level {bg_level}\",\n",
    "        )\n",
    "\n",
    "        per_lambda_stats = html.Div(\n",
    "            [\n",
    "                dcc.Tabs(\n",
    "                    [\n",
    "                        dcc.Tab(\n",
    "                            label=\"Vanilla Ensemble\", children=stats_for_lambda(0),\n",
    "                        ),\n",
    "                        dcc.Tab(label=\"Lambda = 1\", children=stats_for_lambda(1)),\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        tabs_for_bg_level.append(\n",
    "            dcc.Tab(\n",
    "                label=f\"label correlation {l_corr}\",\n",
    "                children=[dcc.Graph(figure=fig), per_lambda_stats],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    top_level_tabs.append(\n",
    "        dcc.Tab(\n",
    "            label=f\"background noise level {bg_level}\",\n",
    "            children=[dcc.Tabs(tabs_for_bg_level)],\n",
    "        )\n",
    "    )\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
    "app.layout = html.Div([dcc.Tabs(top_level_tabs)])\n",
    "app.run_server(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(model: tf.keras.Model):\n",
    "    (feature_extractor, logistic) = model.layers[-2:]\n",
    "    [kernel, bias] = feature_extractor.get_weights()\n",
    "\n",
    "    n_hidden_neurons = kernel.shape[-1]\n",
    "\n",
    "    as_images = np.resize(kernel, (28, 28, 3, n_hidden_neurons))\n",
    "\n",
    "    f, axarr = plt.subplots(n_hidden_neurons, 3, figsize=(15, 15))\n",
    "\n",
    "    f.tight_layout()\n",
    "\n",
    "    for i in range(n_hidden_neurons):\n",
    "        for c in range(3):\n",
    "            for_neuron = as_images[:, :, c, i]\n",
    "\n",
    "            for_neuron = np.abs(for_neuron)\n",
    "\n",
    "            axarr[i, c].imshow(for_neuron, cmap=\"gray\")\n",
    "            axarr[i, c].axis(\"off\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.0001, right=10, left=9.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
