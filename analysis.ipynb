{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import concurrent.futures\n",
    "import argparse\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "import json\n",
    "import gin\n",
    "from typing import Dict, Any, Callable\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import lib_analysis\n",
    "import lib_biased_mnist\n",
    "import lib_toy\n",
    "import lib_problem\n",
    "\n",
    "gin.enter_interactive_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(\n",
    "    dataset: tf.data.Dataset, models: List[tf.keras.Model], batch_size: int\n",
    ") -> tf.Tensor:\n",
    "    Xs = []\n",
    "    true_labels = []\n",
    "    predicted = []\n",
    "    biased_labels = []\n",
    "    div_losses = []\n",
    "    for X, y, y_biased in dataset.shuffle(60_000).batch(batch_size):\n",
    "        Xs.append(X)\n",
    "        (features, ys_pred) = lib_problem.forward(X, y, models)\n",
    "        probabilities = tf.nn.softmax(ys_pred)\n",
    "        true_labels.append(y)\n",
    "        biased_labels.append(y_biased)\n",
    "        predicted.append(probabilities)\n",
    "\n",
    "        div = lib_problem.diversity_loss(features, y, 'unbiased_hsic', 'rbf').numpy()\n",
    "        div_losses.append(div)\n",
    "\n",
    "    return Xs, true_labels, biased_labels, predicted, div_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file('config.gin')\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "problem = lib_biased_mnist.BiasedMnistProblem()\n",
    "\n",
    "in_dist = tf.data.Dataset.from_tensor_slices(\n",
    "            problem.filter_tensors(*lib_biased_mnist.get_biased_mnist_data(\"~/.datasets/mnist/\", 1.0, train=False))).cache()\n",
    "\n",
    "oo_dist = tf.data.Dataset.from_tensor_slices(\n",
    "            problem.filter_tensors(*lib_biased_mnist.get_biased_mnist_data(\"~/.datasets/mnist/\", 0.0, train=False))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df: pd.DataFrame, batch_size: int):\n",
    "    root = Path(DF['original_config'][0]).parent\n",
    "    models = []\n",
    "    \n",
    "    for p in df['model_paths'][0]:\n",
    "        p = Path(p)\n",
    "        if not p.is_absolute():\n",
    "            p = root / p\n",
    "        m = tf.keras.models.load_model(p, compile=False)\n",
    "        models.append(m) \n",
    "\n",
    "    in_X, in_y, in_y_biased, in_y_hat, in_d_loss = process_dataset(in_dist, models, batch_size)\n",
    "    oo_X, oo_y, oo_y_biased, oo_y_hat, oo_d_loss = process_dataset(oo_dist, models, batch_size)\n",
    "\n",
    "    dfs = [\n",
    "    {\n",
    "        'diversity': np.asarray(in_d_loss),\n",
    "        'source': 'id'\n",
    "    }, \n",
    "    {\n",
    "        'diversity': np.asarray(oo_d_loss),\n",
    "        'source': 'ood'\n",
    "    }]\n",
    "    df = pd.concat(map(pd.DataFrame, dfs))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_histogram(df: pd.DataFrame, batch_size: int):\n",
    "    fig = go.Figure()\n",
    "    for s in df.source.unique():\n",
    "        sel_df = df[df[\"source\"] == s]\n",
    "        fig.add_trace(go.Histogram(x=sel_df.diversity.to_numpy(), name=s))\n",
    "    fig.update_layout(\n",
    "        title=f\"distribution of diversity for batch size {batch_size}\",\n",
    "        yaxis_type=\"log\",\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_prediction_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_diversity_loss</th>\n",
       "      <th>test_combined_loss</th>\n",
       "      <th>train_prediction_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_diversity_loss</th>\n",
       "      <th>train_combined_loss</th>\n",
       "      <th>name</th>\n",
       "      <th>model_paths</th>\n",
       "      <th>kernel</th>\n",
       "      <th>indep</th>\n",
       "      <th>lambda</th>\n",
       "      <th>label_correlation</th>\n",
       "      <th>BiasedMnistProblem.filter_for_digits</th>\n",
       "      <th>BiasedMnistProblem.model_type</th>\n",
       "      <th>Problem.initial_lr</th>\n",
       "      <th>Problem.n_epochs</th>\n",
       "      <th>original_config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[878.8961181640625, 1478.6832275390625]</td>\n",
       "      <td>[0.24539999663829803, 0.22269999980926514]</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>2357.579102</td>\n",
       "      <td>[0.13799864053726196, 0.39479386806488037]</td>\n",
       "      <td>[0.9991000294685364, 0.9985166788101196]</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.536194</td>\n",
       "      <td>biased_mnist_problem</td>\n",
       "      <td>[model-0.h5, model-1.h5]</td>\n",
       "      <td>rbf</td>\n",
       "      <td>conditional_hsic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>mlp</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>../data/one-off/config.yaml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      test_prediction_loss  \\\n",
       "0  [878.8961181640625, 1478.6832275390625]   \n",
       "\n",
       "                                test_accuracy  test_diversity_loss  \\\n",
       "0  [0.24539999663829803, 0.22269999980926514]            -0.000011   \n",
       "\n",
       "   test_combined_loss                       train_prediction_loss  \\\n",
       "0         2357.579102  [0.13799864053726196, 0.39479386806488037]   \n",
       "\n",
       "                             train_accuracy  train_diversity_loss  \\\n",
       "0  [0.9991000294685364, 0.9985166788101196]              0.000106   \n",
       "\n",
       "   train_combined_loss                  name               model_paths kernel  \\\n",
       "0             0.536194  biased_mnist_problem  [model-0.h5, model-1.h5]    rbf   \n",
       "\n",
       "              indep  lambda  label_correlation  \\\n",
       "0  conditional_hsic       1              0.999   \n",
       "\n",
       "  BiasedMnistProblem.filter_for_digits BiasedMnistProblem.model_type  \\\n",
       "0                               [0, 1]                           mlp   \n",
       "\n",
       "   Problem.initial_lr  Problem.n_epochs              original_config  \n",
       "0                0.01               100  ../data/one-off/config.yaml  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for bs in [4, 8, 16, 32, 64, 128, 256, 512]:\n",
    "    DF = lib_analysis.read_problem(Path('../data/one-off'), '.').head(1)\n",
    "    df = process_df(DF, bs)\n",
    "    make_histogram(df, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = lib_analysis.read_problem(Path('../data'), 'biased_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_corr = 0.999\n",
    "\n",
    "for batch_size in [4, 8, 16, 32, 64, 128, 256, 512]:\n",
    "    df = DF[\n",
    "        (DF['lambda'] == 1) &\n",
    "        (DF['label_correlation'] == label_corr) &\n",
    "        (DF['indep'] == 'unbiased_hsic')\n",
    "    ].reset_index(drop=True)\n",
    "    df_hsic = process_df(df, batch_size)\n",
    "\n",
    "    df = DF[\n",
    "        (DF['lambda'] == 0) &\n",
    "        (DF['label_correlation'] == label_corr) &\n",
    "        (DF['indep'] == 'unbiased_hsic')\n",
    "    ].reset_index(drop=True)\n",
    "    df_vanilla = process_df(df, batch_size)\n",
    "\n",
    "    df_hsic['source'] = 'hsic-' + df_hsic['source']\n",
    "    df_vanilla['source'] = 'vanilla-' + df_vanilla['source']\n",
    "\n",
    "    df = pd.concat([df_hsic, df_vanilla])\n",
    "    make_histogram(df, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(model: tf.keras.Model):\n",
    "    (feature_extractor, logistic) = model.layers[-2:]\n",
    "    [kernel, bias] = feature_extractor.get_weights()\n",
    "    \n",
    "    n_hidden_neurons = kernel.shape[-1]\n",
    "    \n",
    "    as_images = np.resize(kernel, (28, 28, 3, n_hidden_neurons))\n",
    "    \n",
    "    f, axarr = plt.subplots(n_hidden_neurons, 3, figsize=(15, 15))\n",
    "    \n",
    "    f.tight_layout()\n",
    "    \n",
    "    for i in range(n_hidden_neurons):\n",
    "        for c in range(3):\n",
    "            for_neuron = as_images[:, :, c, i]\n",
    "            \n",
    "            for_neuron = np.abs(for_neuron)\n",
    "\n",
    "            axarr[i, c].imshow(for_neuron, cmap='gray')\n",
    "            axarr[i, c].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.0001, right=10, left=9.3)\n",
    "    plt.show()\n",
    "\n",
    "print('Model 0')\n",
    "visualize_weights(models[0])\n",
    "\n",
    "print('Model 1')\n",
    "visualize_weights(models[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
