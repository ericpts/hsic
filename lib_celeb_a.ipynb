{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import plotly.express as px\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = tfds.load(\"celeb_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_test, D_train, D_val = dset[\"test\"], dset[\"train\"], dset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: {attributes: {5_o_Clock_Shadow: (), Arched_Eyebrows: (), Attractive: (), Bags_Under_Eyes: (), Bald: (), Bangs: (), Big_Lips: (), Big_Nose: (), Black_Hair: (), Blond_Hair: (), Blurry: (), Brown_Hair: (), Bushy_Eyebrows: (), Chubby: (), Double_Chin: (), Eyeglasses: (), Goatee: (), Gray_Hair: (), Heavy_Makeup: (), High_Cheekbones: (), Male: (), Mouth_Slightly_Open: (), Mustache: (), Narrow_Eyes: (), No_Beard: (), Oval_Face: (), Pale_Skin: (), Pointy_Nose: (), Receding_Hairline: (), Rosy_Cheeks: (), Sideburns: (), Smiling: (), Straight_Hair: (), Wavy_Hair: (), Wearing_Earrings: (), Wearing_Hat: (), Wearing_Lipstick: (), Wearing_Necklace: (), Wearing_Necktie: (), Young: ()}, image: (218, 178, 3), landmarks: {lefteye_x: (), lefteye_y: (), leftmouth_x: (), leftmouth_y: (), nose_x: (), nose_y: (), righteye_x: (), righteye_y: (), rightmouth_x: (), rightmouth_y: ()}}, types: {attributes: {5_o_Clock_Shadow: tf.bool, Arched_Eyebrows: tf.bool, Attractive: tf.bool, Bags_Under_Eyes: tf.bool, Bald: tf.bool, Bangs: tf.bool, Big_Lips: tf.bool, Big_Nose: tf.bool, Black_Hair: tf.bool, Blond_Hair: tf.bool, Blurry: tf.bool, Brown_Hair: tf.bool, Bushy_Eyebrows: tf.bool, Chubby: tf.bool, Double_Chin: tf.bool, Eyeglasses: tf.bool, Goatee: tf.bool, Gray_Hair: tf.bool, Heavy_Makeup: tf.bool, High_Cheekbones: tf.bool, Male: tf.bool, Mouth_Slightly_Open: tf.bool, Mustache: tf.bool, Narrow_Eyes: tf.bool, No_Beard: tf.bool, Oval_Face: tf.bool, Pale_Skin: tf.bool, Pointy_Nose: tf.bool, Receding_Hairline: tf.bool, Rosy_Cheeks: tf.bool, Sideburns: tf.bool, Smiling: tf.bool, Straight_Hair: tf.bool, Wavy_Hair: tf.bool, Wearing_Earrings: tf.bool, Wearing_Hat: tf.bool, Wearing_Lipstick: tf.bool, Wearing_Necklace: tf.bool, Wearing_Necktie: tf.bool, Young: tf.bool}, image: tf.uint8, landmarks: {lefteye_x: tf.int64, lefteye_y: tf.int64, leftmouth_x: tf.int64, leftmouth_y: tf.int64, nose_x: tf.int64, nose_y: tf.int64, righteye_x: tf.int64, righteye_y: tf.int64, rightmouth_x: tf.int64, rightmouth_y: tf.int64}}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_biased_hair_and_gender(example: Dict) -> bool:\n",
    "    blonde_female = tf.logical_and(\n",
    "        example[\"attributes\"][\"Blond_Hair\"],\n",
    "        tf.logical_not(example[\"attributes\"][\"Male\"]),\n",
    "    )\n",
    "    brunette_male = tf.logical_and(\n",
    "        example[\"attributes\"][\"Black_Hair\"], example[\"attributes\"][\"Male\"],\n",
    "    )\n",
    "    return tf.logical_or(blonde_female, brunette_male)\n",
    "\n",
    "\n",
    "def extract_image_and_label(example):\n",
    "    X = example[\"image\"]\n",
    "    y = example[\"attributes\"][\"Male\"]\n",
    "    y_biased = example[\"attributes\"][\"Blond_Hair\"]\n",
    "\n",
    "    X = tf.image.convert_image_dtype(X, dtype=tf.float32, saturate=False)\n",
    "    return (X, y)\n",
    "\n",
    "\n",
    "def normalize_image(image: tf.Tensor, mean, stddev):\n",
    "    per_channel = []\n",
    "    for i, (m, s) in enumerate(zip(mean, stddev)):\n",
    "        per_channel.append((image[:, :, :, i] - m) / s)\n",
    "    return tf.stack(per_channel, axis=-1)\n",
    "\n",
    "\n",
    "def transform_image(X, y):\n",
    "    orig_w = 178\n",
    "    orig_h = 218\n",
    "    orig_min_dim = min(orig_w, orig_h)\n",
    "    target_resolution = (224, 224)\n",
    "\n",
    "    X = tf.keras.layers.experimental.preprocessing.CenterCrop(\n",
    "        orig_min_dim, orig_min_dim\n",
    "    )(X)\n",
    "    X = tf.keras.layers.experimental.preprocessing.Resizing(*target_resolution)(X)\n",
    "    X = normalize_image(X, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "D_test, D_train, D_val = dset[\"test\"], dset[\"train\"], dset[\"validation\"]\n",
    "D_train = D_train.filter(filter_for_biased_hair_and_gender)\n",
    "D_train = D_train.map(extract_image_and_label)\n",
    "D_train = D_train.batch(1).map(transform_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 2.14616     1.7983196   1.3850982 ]\n",
      "   [ 2.181709    1.8346622   1.4212793 ]\n",
      "   [ 2.1892014   1.8423216   1.4289047 ]\n",
      "   ...\n",
      "   [ 1.8221663   1.3540018   0.88151973]\n",
      "   [ 1.8721639   1.4306725   0.94936836]\n",
      "   [ 1.8721639   1.4306725   0.94936836]]\n",
      "\n",
      "  [[ 2.1343102   1.7862054   1.3730379 ]\n",
      "   [ 2.1698594   1.822548    1.409219  ]\n",
      "   [ 2.1831176   1.8361022   1.4227129 ]\n",
      "   ...\n",
      "   [ 1.8336984   1.3471332   0.8808734 ]\n",
      "   [ 1.8648635   1.3980616   0.92524755]\n",
      "   [ 1.8484645   1.4064441   0.92524755]]\n",
      "\n",
      "  [[ 2.1290352   1.7808126   1.367669  ]\n",
      "   [ 2.1645844   1.8171551   1.4038501 ]\n",
      "   [ 2.1804094   1.8333336   1.4199566 ]\n",
      "   ...\n",
      "   [ 1.797167    1.3014803   0.83817977]\n",
      "   [ 1.8250822   1.338325    0.87210405]\n",
      "   [ 1.8129153   1.344544    0.87210405]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.7754089  -1.7205882  -1.403573  ]\n",
      "   [-1.7872585  -1.7327025  -1.4156334 ]\n",
      "   [-1.7925336  -1.7380952  -1.4210021 ]\n",
      "   ...\n",
      "   [-1.6879866  -1.7747185  -1.5514616 ]\n",
      "   [-1.6832107  -1.7327021  -1.4568597 ]\n",
      "   [-1.7069099  -1.7205882  -1.4210021 ]]\n",
      "\n",
      "  [[-1.7754089  -1.7205882  -1.403573  ]\n",
      "   [-1.7872585  -1.7327025  -1.4156334 ]\n",
      "   [-1.7925336  -1.7380952  -1.4210021 ]\n",
      "   ...\n",
      "   [-1.6856744  -1.7411157  -1.482198  ]\n",
      "   [-1.685236   -1.7235776  -1.4323235 ]\n",
      "   [-1.7016346  -1.7151952  -1.4156331 ]]\n",
      "\n",
      "  [[-1.7754089  -1.7205882  -1.403573  ]\n",
      "   [-1.7872585  -1.7327025  -1.4156334 ]\n",
      "   [-1.7925336  -1.7380952  -1.4210021 ]\n",
      "   ...\n",
      "   [-1.7897805  -1.8053086  -1.5223082 ]\n",
      "   [-1.6897851  -1.7030813  -1.403573  ]\n",
      "   [-1.6897851  -1.7030813  -1.403573  ]]]], shape=(1, 224, 224, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for X, y in D_train:\n",
    "    print(X)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def filter_dataset(D: tf.data.Dataset, attribute_name: str, target_value):\n",
    "    def f_filter(example: Dict) -> bool:\n",
    "        return tf.equal(example[\"attributes\"][attribute_name], target_value)\n",
    "\n",
    "    return D.filter(f_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 10\n",
    "for example in filter_dataset(D_train, \"Male\", True):\n",
    "    attributes, image, landmarks = (\n",
    "        example[\"attributes\"],\n",
    "        example[\"image\"],\n",
    "        example[\"landmarks\"],\n",
    "    )\n",
    "    fig = px.imshow(image)\n",
    "    fig.show()\n",
    "    n_examples -= 1\n",
    "    if n_examples == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_model():\n",
    "    return tf.keras.applications.ResNet50(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_celeb_a_model():\n",
    "    inputs = tf.keras.layers.Input((224, 224, 3))\n",
    "    X = inputs\n",
    "    X = resnet_model()(X)\n",
    "    features = tf.keras.layers.Dense(\n",
    "        100, kernel_regularizer=tf.keras.regularizers.l2(0.0001)\n",
    "    )(X)\n",
    "\n",
    "    X = features\n",
    "    X = tf.keras.layers.Dense(2, kernel_regularizer=tf.keras.regularizers.l2(0.0001))(X)\n",
    "    return tf.keras.Model(inputs, outputs=[features, X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_celeb_a_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
