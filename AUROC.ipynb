{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ericpts/.local/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import gin\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import yaml\n",
    "from jupyter_dash import JupyterDash\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lib_analysis\n",
    "import lib_biased_mnist\n",
    "import lib_plot\n",
    "import lib_problem\n",
    "import lib_toy\n",
    "\n",
    "external_stylesheets = [\"https://codepen.io/chriddyp/pen/bWLwgP.css\"]\n",
    "\n",
    "tqdm.pandas()\n",
    "gin.enter_interactive_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(data_root: Path, experiment_name: str) -> pd.DataFrame:\n",
    "    df_path = data_root / experiment_name / \"df.pickle\"\n",
    "    if df_path.exists():\n",
    "        print(f\"Found cached df at {df_path}. Reusing...\", flush=True)\n",
    "        return pd.read_pickle(str(df_path))\n",
    "\n",
    "    DF = lib_analysis.read_problem(data_root, experiment_name)\n",
    "    DF = lib_analysis.add_statistics_to_df(DF)\n",
    "\n",
    "    DF.to_pickle(str(df_path), protocol=4)\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached df at ../data/biased_mnist/df.pickle. Reusing...\n"
     ]
    }
   ],
   "source": [
    "data_root = Path(\"../data/\")\n",
    "DF = read_df(data_root, \"biased_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ensemble_distribution(y_hat):\n",
    "    m_1, m_2 = y_hat[:, 0, :], y_hat[:, 1, :]\n",
    "    y_hat_ensemble = (m_1 + m_2) / 2\n",
    "    return y_hat_ensemble\n",
    "\n",
    "\n",
    "def pmax(y_hat_ensemble, _y_hat):\n",
    "    return tf.math.reduce_max(y_hat_ensemble, axis=1)\n",
    "\n",
    "\n",
    "def entropy(y_hat_ensemble, _y_hat):\n",
    "    return -tf.math.reduce_sum(\n",
    "        y_hat_ensemble * tf.math.log(y_hat_ensemble + tf.keras.backend.epsilon()),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def max_diff(y_hat_ensemble, y_hat):\n",
    "    per_model = []\n",
    "    for im in range(2):\n",
    "        per_model.append(\n",
    "            tf.math.reduce_max(tf.math.abs(y_hat_ensemble - y_hat[:, im, :]), axis=1)\n",
    "        )\n",
    "    ret = tf.math.reduce_max(per_model, axis=0)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def average_diff(y_hat_ensemble, y_hat):\n",
    "    per_model = []\n",
    "    for im in range(2):\n",
    "        per_model.append(\n",
    "            tf.math.reduce_mean(tf.math.abs(y_hat_ensemble - y_hat[:, im, :]), axis=1)\n",
    "        )\n",
    "    ret = tf.math.reduce_mean(per_model, axis=0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auroc(in_y_hat, oo_y_hat, f_ensemble_score, higher_score_is_ood=True):\n",
    "    id_score = f_ensemble_score(compute_ensemble_distribution(in_y_hat), in_y_hat)\n",
    "    oo_score = f_ensemble_score(compute_ensemble_distribution(oo_y_hat), oo_y_hat)\n",
    "\n",
    "    if higher_score_is_ood:\n",
    "        id_label, oo_label = 0, 1\n",
    "    else:\n",
    "        id_label, oo_label = 1, 0\n",
    "\n",
    "    y_true = tf.convert_to_tensor(\n",
    "        ([id_label] * id_score.shape[0]) + ([oo_label] * oo_score.shape[0])\n",
    "    )\n",
    "    y_score = tf.concat([id_score, oo_score], axis=0)\n",
    "    return sklearn.metrics.roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(in_y_hat, oo_y_hat):\n",
    "    return {\n",
    "        \"auroc_pmax\": compute_auroc(\n",
    "            in_y_hat, oo_y_hat, pmax, higher_score_is_ood=False\n",
    "        ),\n",
    "        \"auroc_entropy\": compute_auroc(in_y_hat, oo_y_hat, entropy),\n",
    "        \"auroc_max_diff\": compute_auroc(in_y_hat, oo_y_hat, max_diff),\n",
    "        \"auroc_average_diff\": compute_auroc(in_y_hat, oo_y_hat, average_diff),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_for_row(row: pd.Series):\n",
    "    gin.parse_config_file(row[\"gin_config_file\"])\n",
    "\n",
    "    problem = lib_biased_mnist.BiasedMnistProblem()\n",
    "    in_dist = problem.generate_id_testing_data(include_bias=True)\n",
    "    oo_dist = problem.generate_ood_testing_data(include_bias=True)\n",
    "\n",
    "    models = []\n",
    "    for p in row[\"model_paths\"]:\n",
    "        m = tf.keras.models.load_model(p, compile=False)\n",
    "        models.append(m)\n",
    "\n",
    "    in_X, in_y, in_y_biased, in_y_hat = lib_analysis.process_dataset(in_dist, models)\n",
    "    oo_X, oo_y, oo_y_biased, oo_y_hat = lib_analysis.process_dataset(oo_dist, models)\n",
    "    return compute_metrics(in_y_hat, oo_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = lib_analysis.add_columns_to_df(\n",
    "    DF, DF.progress_apply(compute_metrics_for_row, axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.to_pickle(\"/home/ericpts/work/data/biased_mnist/df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for lr 0.0001\n",
      "Processing data for lr 0.001\n",
      "Processing data for lr 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4312239a60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_level_tabs = []\n",
    "\n",
    "for lr in np.unique(DF[\"Problem.initial_lr\"]):\n",
    "    print(f\"Processing data for lr {lr}\")\n",
    "    tabs_for_outer = []\n",
    "    for l_corr in np.unique(DF[\"label_correlation\"]):\n",
    "        df_for_fig = DF[\n",
    "            (DF[\"label_correlation\"] == l_corr) & (DF[\"Problem.initial_lr\"] == lr)\n",
    "        ].copy()\n",
    "\n",
    "        figs = []\n",
    "        for c in [\n",
    "            \"pmax\",\n",
    "            \"entropy\",\n",
    "            \"max_diff\",\n",
    "            \"average_diff\",\n",
    "        ]:\n",
    "            figs.append(\n",
    "                lib_plot.end_to_end_plot(\n",
    "                    go.Figure(),\n",
    "                    df_for_fig,\n",
    "                    f\"auroc_{c}\",\n",
    "                    [\"indep\"],\n",
    "                    f\"AUROC({c}) l_corr {l_corr}; learning rate {lr}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        tabs_for_outer.append(\n",
    "            dcc.Tab(\n",
    "                label=f\"label correlation {l_corr}\",\n",
    "                children=[dcc.Graph(figure=fig) for fig in figs],\n",
    "            )\n",
    "        )\n",
    "    top_level_tabs.append(\n",
    "        dcc.Tab(label=f\"learning rate {lr}\", children=[dcc.Tabs(tabs_for_outer)])\n",
    "    )\n",
    "\n",
    "\n",
    "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
    "app.layout = html.Div([dcc.Tabs(top_level_tabs)])\n",
    "app.run_server(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
